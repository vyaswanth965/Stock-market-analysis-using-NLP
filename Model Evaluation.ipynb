{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import roc_auc_score,log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_predict,train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "import gensim\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold,KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression,LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "\n",
    "from sklearn.metrics import log_loss, accuracy_score,mean_squared_error,mean_absolute_error,r2_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.externals import joblib\n",
    "from scikitplot.metrics import plot_confusion_matrix\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import json\n",
    "import gensim\n",
    "\n",
    "import scikitplot.plotters as skplt\n",
    "\n",
    "import nltk\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date company                                               news  \\\n",
      "0 2019-01-14     HCL  HCL Tech Q3 PAT seen up 4.2% QoQ to Rs. 2,629....   \n",
      "1 2018-12-07     HCL  HCL Technologies-IBM deal fails to enthuse inv...   \n",
      "2 2018-12-06     HCL  Technical Views | Top buy & sell ideas by Ashw...   \n",
      "3 2018-11-09     HCL  HCL Technologies Limited Q2 FY’19 Earnings Con...   \n",
      "4 2018-10-29     HCL  Buy HCL Technologies, target Rs 1182: Anand Rathi   \n",
      "5 2018-10-24     HCL  HCL Technologies – good outlook backed by reas...   \n",
      "6 2018-10-23     HCL  HCL Technologies sees 5.7% sequential rise in ...   \n",
      "7 2018-10-22     HCL  HCL Technologies Q2 results on October 23; her...   \n",
      "8 2018-10-15     HCL  Stock Picks of the Day | Nifty may be vulnerab...   \n",
      "9 2018-10-08     HCL  HCL Tech to set up global IT centres in Andhra...   \n",
      "\n",
      "   loss/profit   %change                                     Processed_news  \n",
      "0            0 -0.472399  hcl tech q3 pat see up 4 . 2 % qoq rs . 2,629 ...  \n",
      "1            0 -3.941059  hcl technology - ibm deal fail enthuse investo...  \n",
      "2            0 -3.614630  technical view top buy &amp; sell idea ashwani...  \n",
      "3            0 -0.097087  hcl technology limit q2 fy ’ 19 earning confer...  \n",
      "4            1  2.113990  buy hcl technology , target rs 1182 : anand rathi  \n",
      "5            1  0.775154  hcl technology – good outlook back reasonable ...  \n",
      "6            0 -1.778350  hcl technology see 5 . 7 % sequential rise q2 ...  \n",
      "7            1  2.088542  hcl technology q2 result october 23 ; key thin...  \n",
      "8            1  1.833232  stock pick day nifty may vulnerable short term...  \n",
      "9            0 -0.097674  hcl tech set up global -PRON- centre andhra pr...  \n"
     ]
    }
   ],
   "source": [
    "data = pd.read_excel('C:\\\\Users\\\\shanthi\\\\Desktop\\\\CDAC-DBDA\\\\Project\\\\AllDataProcessed2.xlsx')\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"Processed_news\"]\n",
    "y = data[\"loss/profit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to call various Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_features(X, y, clf=None):\n",
    "    \"\"\"General helper function for evaluating effectiveness of passed features in ML model\n",
    "    \n",
    "    Prints out Log loss, accuracy, and confusion matrix with 3-fold stratified cross-validation\n",
    "    \n",
    "    Args:\n",
    "        X (array-like): Features array. Shape (n_samples, n_features)\n",
    "        \n",
    "        y (array-like): Labels array. Shape (n_samples,)\n",
    "        \n",
    "        clf: Classifier to use. If None, default Log reg is use.\n",
    "    \"\"\"\n",
    "    if clf is None:\n",
    "        clf = LogisticRegression()\n",
    "    \n",
    "    probas = cross_val_predict(clf, X, y, cv=KFold(random_state=2019,n_splits=6), \n",
    "                              n_jobs=-1, method='predict_proba')\n",
    "    pred_indices = np.argmax(probas, axis=1)\n",
    "    classes = np.unique(y)\n",
    "    preds = classes[pred_indices]\n",
    "    print('Log loss: {}'.format(log_loss(y, probas)))\n",
    "    print('Accuracy: {}'.format(accuracy_score(y, preds)))\n",
    "    print('Classification report',classification_report(y,preds))\n",
    "    print('Confusion Matrix  ', confusion_matrix(y,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58527"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))  \n",
    "\n",
    "bag_of_words = count_vectorizer.fit_transform(data['Processed_news'])\n",
    "    \n",
    "len(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.8019122983958645\n",
      "Accuracy: 0.5449918787222523\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.58      0.57      4738\n",
      "           1       0.53      0.51      0.52      4497\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      9235\n",
      "   macro avg       0.54      0.54      0.54      9235\n",
      "weighted avg       0.54      0.54      0.54      9235\n",
      "\n",
      "Confusion Matrix   [[2732 2006]\n",
      " [2196 2301]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(bag_of_words, data['loss/profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.6895947799462351\n",
      "Accuracy: 0.5378451543042772\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.82      0.65      4738\n",
      "           1       0.56      0.24      0.33      4497\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      9235\n",
      "   macro avg       0.55      0.53      0.49      9235\n",
      "weighted avg       0.55      0.54      0.49      9235\n",
      "\n",
      "Confusion Matrix   [[3905  833]\n",
      " [3435 1062]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(bag_of_words, data['loss/profit'], \n",
    "                  RandomForestClassifier(n_estimators=300, max_depth=30,random_state=2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_features(bag_of_words, data['loss/profit'],SVC(kernel='linear',probability=True,random_state=2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1749"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = TfidfVectorizer()    \n",
    "\n",
    "tfidf = count_vectorizer.fit_transform(data['Processed_news'])\n",
    "\n",
    "len(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.7009660189832868\n",
      "Accuracy: 0.5378451543042772\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.59      0.57      4738\n",
      "           1       0.53      0.48      0.50      4497\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      9235\n",
      "   macro avg       0.54      0.54      0.54      9235\n",
      "weighted avg       0.54      0.54      0.54      9235\n",
      "\n",
      "Confusion Matrix   [[2792 1946]\n",
      " [2322 2175]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(tfidf, data['loss/profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.6894071945901016\n",
      "Accuracy: 0.5356794802382241\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.72      0.62      4738\n",
      "           1       0.54      0.34      0.41      4497\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      9235\n",
      "   macro avg       0.54      0.53      0.51      9235\n",
      "weighted avg       0.54      0.54      0.52      9235\n",
      "\n",
      "Confusion Matrix   [[3434 1304]\n",
      " [2984 1513]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(tfidf, data['loss/profit'], SVC(kernel='linear', probability=True, random_state=2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.6897448911893616\n",
      "Accuracy: 0.5431510557661072\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.78      0.64      4738\n",
      "           1       0.56      0.30      0.39      4497\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      9235\n",
      "   macro avg       0.55      0.54      0.51      9235\n",
      "weighted avg       0.55      0.54      0.51      9235\n",
      "\n",
      "Confusion Matrix   [[3689 1049]\n",
      " [3170 1327]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(tfidf, data['loss/profit'], SVC(kernel='rbf', probability=True, random_state=45,C=2,gamma=.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8907, 58111)\n"
     ]
    }
   ],
   "source": [
    "basictf= TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),sublinear_tf=True)\n",
    "basicTrain=basictf.fit_transform(X_train)\n",
    "print(basicTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(basicTrain, label=y_train)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.002\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'binary'\n",
    "params['metric'] = 'binary_logloss'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 32\n",
    "params['min_data'] = 50\n",
    "params['max_depth'] = 20\n",
    "params['max_bin']=1024\n",
    "clf = lgb.train(params, d_train,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42222875, 0.48595289, 0.44494029, ..., 0.51849818, 0.41840873,\n",
       "       0.48488979])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basicTest = basictf.transform(X_test)\n",
    "y_pred = clf.predict(basicTest)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2227):\n",
    "    if y_pred[i]>=.5:       # setting threshold to .5\n",
    "       y_pred[i]=1\n",
    "    else:  \n",
    "       y_pred[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[770 366]\n",
      " [653 438]]\n",
      "0.5424337674000899\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location):\n",
    "    \"\"\"Returns trained word2vec\n",
    "    \n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=1000, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found w2vmodel\n"
     ]
    }
   ],
   "source": [
    "w2vec = get_word2vec(\n",
    "    MySentences(\n",
    "        data['Processed_news'].values, \n",
    "    ),\n",
    "    'w2vmodel'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):w2vec = get_word2vec(\n",
    "    MySentences(\n",
    "        data['Processed_news'].values, \n",
    "    ),\n",
    "    'w2vmodel'\n",
    ")\n",
    "    pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanthi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found w2vmodel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.05764131,  0.03496192,  0.02898565, ...,  0.02233779,\n",
       "        -0.03433947,  0.07995956],\n",
       "       [-0.05459464,  0.03316778,  0.02726857, ...,  0.02107938,\n",
       "        -0.03246247,  0.07540307],\n",
       "       [-0.06403969,  0.03862501,  0.03191711, ...,  0.02499009,\n",
       "        -0.03799589,  0.08837906],\n",
       "       ...,\n",
       "       [-0.04040901,  0.02406988,  0.0200002 , ...,  0.01590047,\n",
       "        -0.02378608,  0.05545349],\n",
       "       [-0.0430277 ,  0.02577958,  0.02139688, ...,  0.01666953,\n",
       "        -0.02555102,  0.05874644],\n",
       "       [-0.0565276 ,  0.03399789,  0.02814336, ...,  0.02198508,\n",
       "        -0.03353221,  0.07792477]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\n",
    "mean_embedded = mean_embedding_vectorizer.fit_transform(data['Processed_news'])\n",
    "mean_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.6928161544595319\n",
      "Accuracy: 0.5127233351380617\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.98      0.67      4738\n",
      "           1       0.49      0.02      0.04      4497\n",
      "\n",
      "   micro avg       0.51      0.51      0.51      9235\n",
      "   macro avg       0.50      0.50      0.36      9235\n",
      "weighted avg       0.50      0.51      0.37      9235\n",
      "\n",
      "Confusion Matrix   [[4641   97]\n",
      " [4403   94]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(mean_embedded, data['loss/profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.6999228327191709\n",
      "Accuracy: 0.5206280454791554\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.60      0.56      4738\n",
      "           1       0.51      0.44      0.47      4497\n",
      "\n",
      "   micro avg       0.52      0.52      0.52      9235\n",
      "   macro avg       0.52      0.52      0.52      9235\n",
      "weighted avg       0.52      0.52      0.52      9235\n",
      "\n",
      "Confusion Matrix   [[2836 1902]\n",
      " [2525 1972]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(mean_embedded, \n",
    "                  data['loss/profit'],\n",
    "                  XGBClassifier(max_depth=4,\n",
    "                                \n",
    "                                )\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.7293040620212402\n",
      "Accuracy: 0.5204114780725501\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.58      0.56      4738\n",
      "           1       0.51      0.45      0.48      4497\n",
      "\n",
      "   micro avg       0.52      0.52      0.52      9235\n",
      "   macro avg       0.52      0.52      0.52      9235\n",
      "weighted avg       0.52      0.52      0.52      9235\n",
      "\n",
      "Confusion Matrix   [[2762 1976]\n",
      " [2453 2044]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(mean_embedded, data['loss/profit'],RandomForestClassifier(n_estimators=200, max_depth=30, random_state=45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Keras tokenizer\n",
    "num_words = 2000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data['Processed_news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the data \n",
    "X = tokenizer.texts_to_sequences(data['Processed_news'].values)\n",
    "X = pad_sequences(X, maxlen=200)\n",
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 200, 138)          276000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 200)               271200    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 547,602\n",
      "Trainable params: 547,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build out our simple LSTM\n",
    "embed_dim = 138\n",
    "lstm_out = 200\n",
    "\n",
    "# Model saving callback\n",
    "ckpt_callback = ModelCheckpoint('keras_model', \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='auto')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embed_dim, input_length = X.shape[1]))\n",
    "model.add(LSTM( lstm_out,recurrent_dropout=0.2, dropout=0.2,activation='relu'))\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7388, 200) (7388, 2)\n",
      "(1847, 200) (1847, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['loss/profit']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42, stratify=Y)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6649 samples, validate on 739 samples\n",
      "Epoch 1/5\n",
      "6649/6649 [==============================] - 119s 18ms/step - loss: 0.6629 - acc: 0.6630 - val_loss: 0.7977 - val_acc: 0.5034\n",
      "Epoch 2/5\n",
      "6649/6649 [==============================] - 114s 17ms/step - loss: 0.6079 - acc: 0.6745 - val_loss: 0.8322 - val_acc: 0.5189\n",
      "Epoch 3/5\n",
      "6649/6649 [==============================] - 115s 17ms/step - loss: 0.5630 - acc: 0.6860 - val_loss: 0.9357 - val_acc: 0.5108\n",
      "Epoch 4/5\n",
      "6649/6649 [==============================] - 115s 17ms/step - loss: 0.5302 - acc: 0.7033 - val_loss: 1.0672 - val_acc: 0.5027\n",
      "Epoch 5/5\n",
      "6649/6649 [==============================] - 115s 17ms/step - loss: 0.4976 - acc: 0.7282 - val_loss: 1.1746 - val_acc: 0.5149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22db2c50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs=5, batch_size=batch_size, validation_split=0.1, verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('keras_model')\n",
    "model = load_model('keras_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 1 1]\n",
      "[1 2 2 ... 2 2 2]\n",
      "Log loss: 1.0431785102003754\n",
      "Accuracy: 0.5230102869518137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[469, 479],\n",
       "       [402, 497]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_indices = np.argmax(probas, axis=1)\n",
    "print(pred_indices)\n",
    "classes = np.array(range(1,3))\n",
    "preds = classes[pred_indices]\n",
    "print(preds)\n",
    "print('Log loss: {}'.format(log_loss(classes[np.argmax(Y_test, axis=1)], probas)))\n",
    "print('Accuracy: {}'.format(accuracy_score(classes[np.argmax(Y_test, axis=1)], preds)))\n",
    "confusion_matrix(classes[np.argmax(Y_test, axis=1)], preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
