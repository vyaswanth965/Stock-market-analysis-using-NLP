{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.6.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs: \n",
      "    - keras\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    openssl-1.0.2p             |    h14c3975_1002         3.1 MB  conda-forge\n",
      "    keras-2.2.4                |           py36_0         456 KB  conda-forge\n",
      "    astor-0.7.1                |             py_0          22 KB  conda-forge\n",
      "    gast-0.2.1.post0           |             py_0           9 KB  conda-forge\n",
      "    pygpu-0.7.6                |py36h3010b51_1000         657 KB  conda-forge\n",
      "    c-ares-1.15.0              |    h14c3975_1001          98 KB  conda-forge\n",
      "    theano-1.0.3               |           py36_0         3.7 MB  conda-forge\n",
      "    mako-1.0.7                 |             py_1          57 KB  conda-forge\n",
      "    keras-applications-1.0.4   |             py_1          26 KB  conda-forge\n",
      "    tensorboard-1.10.0         |           py36_0         3.3 MB  conda-forge\n",
      "    grpcio-1.16.0              |   py36hd60e7a3_0         4.9 MB  conda-forge\n",
      "    libgpuarray-0.7.6          |    h14c3975_1003         263 KB  conda-forge\n",
      "    tensorflow-1.10.0          |           py36_0        50.4 MB  conda-forge\n",
      "    keras-preprocessing-1.0.2  |             py_1          26 KB  conda-forge\n",
      "    markdown-2.6.11            |             py_0          56 KB  conda-forge\n",
      "    absl-py-0.7.0              |        py36_1000         154 KB  conda-forge\n",
      "    termcolor-1.1.0            |             py_2           6 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        67.2 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    absl-py:             0.7.0-py36_1000         conda-forge\n",
      "    astor:               0.7.1-py_0              conda-forge\n",
      "    c-ares:              1.15.0-h14c3975_1001    conda-forge\n",
      "    gast:                0.2.1.post0-py_0        conda-forge\n",
      "    grpcio:              1.16.0-py36hd60e7a3_0   conda-forge\n",
      "    keras:               2.2.4-py36_0            conda-forge\n",
      "    keras-applications:  1.0.4-py_1              conda-forge\n",
      "    keras-preprocessing: 1.0.2-py_1              conda-forge\n",
      "    libgpuarray:         0.7.6-h14c3975_1003     conda-forge\n",
      "    mako:                1.0.7-py_1              conda-forge\n",
      "    markdown:            2.6.11-py_0             conda-forge\n",
      "    pygpu:               0.7.6-py36h3010b51_1000 conda-forge\n",
      "    tensorboard:         1.10.0-py36_0           conda-forge\n",
      "    tensorflow:          1.10.0-py36_0           conda-forge\n",
      "    termcolor:           1.1.0-py_2              conda-forge\n",
      "    theano:              1.0.3-py36_0            conda-forge\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    ca-certificates:     2018.8.24-ha4d7672_0    conda-forge --> 2018.11.29-ha4d7672_0 conda-forge\n",
      "    certifi:             2018.8.24-py36_1        conda-forge --> 2018.11.29-py36_1000  conda-forge\n",
      "    openssl:             1.0.2p-h470a237_0       conda-forge --> 1.0.2p-h14c3975_1002  conda-forge\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "openssl-1.0.2p       | 3.1 MB    | ##################################### | 100% \n",
      "keras-2.2.4          | 456 KB    | ##################################### | 100% \n",
      "astor-0.7.1          | 22 KB     | ##################################### | 100% \n",
      "gast-0.2.1.post0     | 9 KB      | ##################################### | 100% \n",
      "pygpu-0.7.6          | 657 KB    | ##################################### | 100% \n",
      "c-ares-1.15.0        | 98 KB     | ##################################### | 100% \n",
      "theano-1.0.3         | 3.7 MB    | ##################################### | 100% \n",
      "mako-1.0.7           | 57 KB     | ##################################### | 100% \n",
      "keras-applications-1 | 26 KB     | ##################################### | 100% \n",
      "tensorboard-1.10.0   | 3.3 MB    | ##################################### | 100% \n",
      "grpcio-1.16.0        | 4.9 MB    | ##################################### | 100% \n",
      "libgpuarray-0.7.6    | 263 KB    | ##################################### | 100% \n",
      "tensorflow-1.10.0    | 50.4 MB   | ##################################### | 100% \n",
      "keras-preprocessing- | 26 KB     | ##################################### | 100% \n",
      "markdown-2.6.11      | 56 KB     | ##################################### | 100% \n",
      "absl-py-0.7.0        | 154 KB    | ##################################### | 100% \n",
      "termcolor-1.1.0      | 6 KB      | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -n python3 -y -c conda-forge keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.5.11\n",
      "  latest version: 4.6.1\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/ec2-user/anaconda3/envs/python3\n",
      "\n",
      "  added / updated specs: \n",
      "    - xgboost\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    xgboost-0.81               |py36hf484d3e_1000           9 KB  conda-forge\n",
      "    _py-xgboost-mutex-2.0      |            cpu_0           8 KB  conda-forge\n",
      "    libstdcxx-ng-7.3.0         |       hdf63c60_0         2.6 MB  conda-forge\n",
      "    py-xgboost-0.81            |py36hf484d3e_1000          66 KB  conda-forge\n",
      "    libxgboost-0.81            |    hf484d3e_1000         3.8 MB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         6.4 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "    _py-xgboost-mutex: 2.0-cpu_0              conda-forge\n",
      "    libxgboost:        0.81-hf484d3e_1000     conda-forge\n",
      "    py-xgboost:        0.81-py36hf484d3e_1000 conda-forge\n",
      "    xgboost:           0.81-py36hf484d3e_1000 conda-forge\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "    libstdcxx-ng:      7.2.0-hdf63c60_3                   --> 7.3.0-hdf63c60_0 conda-forge\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "xgboost-0.81         | 9 KB      | ##################################### | 100% \n",
      "_py-xgboost-mutex-2. | 8 KB      | ##################################### | 100% \n",
      "libstdcxx-ng-7.3.0   | 2.6 MB    | ##################################### | 100% \n",
      "py-xgboost-0.81      | 66 KB     | ##################################### | 100% \n",
      "libxgboost-0.81      | 3.8 MB    | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -n python3 -y -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import roc_auc_score,log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split,KFold,cross_val_predict,train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "#import gensim\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date company                                               news  \\\n",
      "0 2019-01-14     HCL  HCL Tech Q3 PAT seen up 4.2% QoQ to Rs. 2,629....   \n",
      "1 2018-12-07     HCL  HCL Technologies-IBM deal fails to enthuse inv...   \n",
      "2 2018-12-06     HCL  Technical Views | Top buy & sell ideas by Ashw...   \n",
      "3 2018-11-09     HCL  HCL Technologies Limited Q2 FY’19 Earnings Con...   \n",
      "4 2018-10-29     HCL  Buy HCL Technologies, target Rs 1182: Anand Rathi   \n",
      "5 2018-10-24     HCL  HCL Technologies – good outlook backed by reas...   \n",
      "6 2018-10-23     HCL  HCL Technologies sees 5.7% sequential rise in ...   \n",
      "7 2018-10-22     HCL  HCL Technologies Q2 results on October 23; her...   \n",
      "8 2018-10-15     HCL  Stock Picks of the Day | Nifty may be vulnerab...   \n",
      "9 2018-10-08     HCL  HCL Tech to set up global IT centres in Andhra...   \n",
      "\n",
      "   loss/profit   %change                                     Processed_news  \n",
      "0            0 -0.472399  hcl tech q3 pat see up 4 . 2 % qoq rs . 2,629 ...  \n",
      "1            0 -3.941059  hcl technology - ibm deal fail enthuse investo...  \n",
      "2            0 -3.614630  technical view top buy &amp; sell idea ashwani...  \n",
      "3            0 -0.097087  hcl technology limit q2 fy ’ 19 earning confer...  \n",
      "4            1  2.113990  buy hcl technology , target rs 1182 : anand rathi  \n",
      "5            1  0.775154  hcl technology – good outlook back reasonable ...  \n",
      "6            0 -1.778350  hcl technology see 5 . 7 % sequential rise q2 ...  \n",
      "7            1  2.088542  hcl technology q2 result october 23 ; key thin...  \n",
      "8            1  1.833232  stock pick day nifty may vulnerable short term...  \n",
      "9            0 -0.097674  hcl tech set up global -PRON- centre andhra pr...  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>company</th>\n",
       "      <th>news</th>\n",
       "      <th>loss/profit</th>\n",
       "      <th>%change</th>\n",
       "      <th>Processed_news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9235</td>\n",
       "      <td>9235</td>\n",
       "      <td>9235</td>\n",
       "      <td>9235.000000</td>\n",
       "      <td>9235.000000</td>\n",
       "      <td>9235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>3078</td>\n",
       "      <td>8</td>\n",
       "      <td>8828</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>2007-09-27 00:00:00</td>\n",
       "      <td>WIPRO</td>\n",
       "      <td>6 trading stocks for short term gain</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6 trading stock short term gain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>8</td>\n",
       "      <td>1371</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>2005-01-03 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>2019-01-18 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.486952</td>\n",
       "      <td>-0.009986</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.499857</td>\n",
       "      <td>2.255059</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-20.791906</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.104271</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.045614</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.069719</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.286659</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date company                                  news  \\\n",
       "count                  9235    9235                                  9235   \n",
       "unique                 3078       8                                  8828   \n",
       "top     2007-09-27 00:00:00   WIPRO  6 trading stocks for short term gain   \n",
       "freq                      8    1371                                     7   \n",
       "first   2005-01-03 00:00:00     NaN                                   NaN   \n",
       "last    2019-01-18 00:00:00     NaN                                   NaN   \n",
       "mean                    NaN     NaN                                   NaN   \n",
       "std                     NaN     NaN                                   NaN   \n",
       "min                     NaN     NaN                                   NaN   \n",
       "25%                     NaN     NaN                                   NaN   \n",
       "50%                     NaN     NaN                                   NaN   \n",
       "75%                     NaN     NaN                                   NaN   \n",
       "max                     NaN     NaN                                   NaN   \n",
       "\n",
       "        loss/profit      %change                   Processed_news  \n",
       "count   9235.000000  9235.000000                             9235  \n",
       "unique          NaN          NaN                             8821  \n",
       "top             NaN          NaN  6 trading stock short term gain  \n",
       "freq            NaN          NaN                                7  \n",
       "first           NaN          NaN                              NaN  \n",
       "last            NaN          NaN                              NaN  \n",
       "mean       0.486952    -0.009986                              NaN  \n",
       "std        0.499857     2.255059                              NaN  \n",
       "min        0.000000   -20.791906                              NaN  \n",
       "25%        0.000000    -1.104271                              NaN  \n",
       "50%        0.000000    -0.045614                              NaN  \n",
       "75%        1.000000     1.069719                              NaN  \n",
       "max        1.000000    20.286659                              NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "bucket='stocknewsstorage'\n",
    "data_key = 'AllDataProcessed2.xlsx'\n",
    "data_location = 's3://{}/{}'.format(bucket, data_key)\n",
    "data = pd.read_excel(data_location)\n",
    "print(data.head(10))\n",
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"Processed_news\"]\n",
    "y = data[\"loss/profit\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to call various Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_features(X, y, clf=None):\n",
    "    \"\"\"General helper function for evaluating effectiveness of passed features in ML model\n",
    "    \n",
    "    Prints out Log loss, accuracy, and confusion matrix with 3-fold stratified cross-validation\n",
    "    \n",
    "    Args:\n",
    "        X (array-like): Features array. Shape (n_samples, n_features)\n",
    "        \n",
    "        y (array-like): Labels array. Shape (n_samples,)\n",
    "        \n",
    "        clf: Classifier to use. If None, default Log reg is use.\n",
    "    \"\"\"\n",
    "    if clf is None:\n",
    "        clf = LogisticRegression()\n",
    "    \n",
    "    probas = cross_val_predict(clf, X, y, cv=KFold(random_state=2019,n_splits=6), \n",
    "                              n_jobs=-1, method='predict_proba')\n",
    "    pred_indices = np.argmax(probas, axis=1)\n",
    "    classes = np.unique(y)\n",
    "    preds = classes[pred_indices]\n",
    "    print('Log loss: {}'.format(log_loss(y, probas)))\n",
    "    print('Accuracy: {}'.format(accuracy_score(y, preds)))\n",
    "    print('Classification report',classification_report(y,preds))\n",
    "    print('Confusion Matrix  ', confusion_matrix(y,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58527"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(ngram_range=(1,2))  \n",
    "\n",
    "bag_of_words = count_vectorizer.fit_transform(data['Processed_news'])\n",
    "    \n",
    "len(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.8019118033858804\n",
      "Accuracy: 0.5449918787222523\n",
      "Classification report              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.58      0.57      4738\n",
      "          1       0.53      0.51      0.52      4497\n",
      "\n",
      "avg / total       0.54      0.54      0.54      9235\n",
      "\n",
      "Confusion Matrix   [[2732 2006]\n",
      " [2196 2301]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(bag_of_words, data['loss/profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.6895947799462351\n",
      "Accuracy: 0.5378451543042772\n",
      "Classification report              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.53      0.82      0.65      4738\n",
      "          1       0.56      0.24      0.33      4497\n",
      "\n",
      "avg / total       0.55      0.54      0.49      9235\n",
      "\n",
      "Confusion Matrix   [[3905  833]\n",
      " [3435 1062]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(bag_of_words, data['loss/profit'], \n",
    "                  RandomForestClassifier(n_estimators=300, max_depth=30,random_state=2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.6887768641156938\n",
      "Accuracy: 0.5458581483486735\n",
      "Classification report              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.76      0.63      4738\n",
      "          1       0.56      0.32      0.41      4497\n",
      "\n",
      "avg / total       0.55      0.55      0.52      9235\n",
      "\n",
      "Confusion Matrix   [[3591 1147]\n",
      " [3047 1450]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(bag_of_words, data['loss/profit'],SVC(kernel='linear',probability=True,random_state=2019))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8391"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = TfidfVectorizer()    \n",
    "\n",
    "tfidf = count_vectorizer.fit_transform(data['Processed_news'])\n",
    "\n",
    "len(count_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.6956253952558793\n",
      "Accuracy: 0.5408770979967515\n",
      "Classification report              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.55      0.60      0.57      4738\n",
      "          1       0.53      0.48      0.50      4497\n",
      "\n",
      "avg / total       0.54      0.54      0.54      9235\n",
      "\n",
      "Confusion Matrix   [[2842 1896]\n",
      " [2344 2153]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(tfidf, data['loss/profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.6886725767043995\n",
      "Accuracy: 0.542609637249594\n",
      "Classification report              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.54      0.73      0.62      4738\n",
      "          1       0.55      0.35      0.42      4497\n",
      "\n",
      "avg / total       0.54      0.54      0.53      9235\n",
      "\n",
      "Confusion Matrix   [[3454 1284]\n",
      " [2940 1557]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(tfidf, data['loss/profit'], SVC(kernel='linear', probability=True, random_state=2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.6897448911893616\n",
      "Accuracy: 0.5431510557661072\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.78      0.64      4738\n",
      "           1       0.56      0.30      0.39      4497\n",
      "\n",
      "   micro avg       0.54      0.54      0.54      9235\n",
      "   macro avg       0.55      0.54      0.51      9235\n",
      "weighted avg       0.55      0.54      0.51      9235\n",
      "\n",
      "Confusion Matrix   [[3689 1049]\n",
      " [3170 1327]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(tfidf, data['loss/profit'], SVC(kernel='rbf', probability=True, random_state=45,C=2,gamma=.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7388, 49998)\n"
     ]
    }
   ],
   "source": [
    "basictf= TfidfVectorizer(use_idf=True, min_df=0.0, max_df=1.0, ngram_range=(1,2),sublinear_tf=True)\n",
    "basicTrain=basictf.fit_transform(X_train)\n",
    "print(basicTrain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_train = lgb.Dataset(basicTrain, label=y_train)\n",
    "params = {}\n",
    "params['learning_rate'] = 0.002\n",
    "params['boosting_type'] = 'gbdt'\n",
    "params['objective'] = 'binary'\n",
    "params['metric'] = 'binary_logloss'\n",
    "params['sub_feature'] = 0.5\n",
    "params['num_leaves'] = 32\n",
    "params['min_data'] = 50\n",
    "params['max_depth'] = 20\n",
    "params['max_bin']=1024\n",
    "clf = lgb.train(params, d_train,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.42222875, 0.48595289, 0.44494029, ..., 0.51849818, 0.41840873,\n",
       "       0.48488979])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basicTest = basictf.transform(X_test)\n",
    "y_pred = clf.predict(basicTest)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2227):\n",
    "    if y_pred[i]>=.5:       # setting threshold to .5\n",
    "       y_pred[i]=1\n",
    "    else:  \n",
    "       y_pred[i]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[770 366]\n",
      " [653 438]]\n",
      "0.5424337674000899\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    \"\"\"MySentences is a generator to produce a list of tokenized sentences \n",
    "    \n",
    "    Takes a list of numpy arrays containing documents.\n",
    "    \n",
    "    Args:\n",
    "        arrays: List of arrays, where each element in the array contains a document.\n",
    "    \"\"\"\n",
    "    def __init__(self, *arrays):\n",
    "        self.arrays = arrays\n",
    " \n",
    "    def __iter__(self):\n",
    "        for array in self.arrays:\n",
    "            for document in array:\n",
    "                for sent in nltk.sent_tokenize(document):\n",
    "                    yield nltk.word_tokenize(sent)\n",
    "\n",
    "def get_word2vec(sentences, location):\n",
    "    \"\"\"Returns trained word2vec\n",
    "    \n",
    "    Args:\n",
    "        sentences: iterator for sentences\n",
    "        \n",
    "        location (str): Path to save/load word2vec\n",
    "    \"\"\"\n",
    "    if os.path.exists(location):\n",
    "        print('Found {}'.format(location))\n",
    "        model = gensim.models.Word2Vec.load(location)\n",
    "        return model\n",
    "    \n",
    "    print('{} not found. training model'.format(location))\n",
    "    model = gensim.models.Word2Vec(sentences, size=1000, window=5, min_count=5, workers=4)\n",
    "    print('Model done training. Saving to disk')\n",
    "    model.save(location)\n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found w2vmodel\n"
     ]
    }
   ],
   "source": [
    "w2vec = get_word2vec(\n",
    "    MySentences(\n",
    "        data['Processed_news'].values, \n",
    "    ),\n",
    "    'w2vmodel'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTokenizer:\n",
    "    def __init__(self):w2vec = get_word2vec(\n",
    "    MySentences(\n",
    "        data['Processed_news'].values, \n",
    "    ),\n",
    "    'w2vmodel'\n",
    ")\n",
    "    pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        transformed_X = []\n",
    "        for document in X:\n",
    "            tokenized_doc = []\n",
    "            for sent in nltk.sent_tokenize(document):\n",
    "                tokenized_doc += nltk.word_tokenize(sent)\n",
    "            transformed_X.append(np.array(tokenized_doc))\n",
    "        return np.array(transformed_X)\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "    \n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(word2vec.wv.syn0[0])\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = MyTokenizer().fit_transform(X)\n",
    "        \n",
    "        return np.array([\n",
    "            np.mean([self.word2vec.wv[w] for w in words if w in self.word2vec.wv]\n",
    "                    or [np.zeros(self.dim)], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "    \n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shanthi\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found w2vmodel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.05764131,  0.03496192,  0.02898565, ...,  0.02233779,\n",
       "        -0.03433947,  0.07995956],\n",
       "       [-0.05459464,  0.03316778,  0.02726857, ...,  0.02107938,\n",
       "        -0.03246247,  0.07540307],\n",
       "       [-0.06403969,  0.03862501,  0.03191711, ...,  0.02499009,\n",
       "        -0.03799589,  0.08837906],\n",
       "       ...,\n",
       "       [-0.04040901,  0.02406988,  0.0200002 , ...,  0.01590047,\n",
       "        -0.02378608,  0.05545349],\n",
       "       [-0.0430277 ,  0.02577958,  0.02139688, ...,  0.01666953,\n",
       "        -0.02555102,  0.05874644],\n",
       "       [-0.0565276 ,  0.03399789,  0.02814336, ...,  0.02198508,\n",
       "        -0.03353221,  0.07792477]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_embedding_vectorizer = MeanEmbeddingVectorizer(w2vec)\n",
    "mean_embedded = mean_embedding_vectorizer.fit_transform(data['Processed_news'])\n",
    "mean_embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.6928161544595319\n",
      "Accuracy: 0.5127233351380617\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.98      0.67      4738\n",
      "           1       0.49      0.02      0.04      4497\n",
      "\n",
      "   micro avg       0.51      0.51      0.51      9235\n",
      "   macro avg       0.50      0.50      0.36      9235\n",
      "weighted avg       0.50      0.51      0.37      9235\n",
      "\n",
      "Confusion Matrix   [[4641   97]\n",
      " [4403   94]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(mean_embedded, data['loss/profit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.6999228327191709\n",
      "Accuracy: 0.5206280454791554\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.60      0.56      4738\n",
      "           1       0.51      0.44      0.47      4497\n",
      "\n",
      "   micro avg       0.52      0.52      0.52      9235\n",
      "   macro avg       0.52      0.52      0.52      9235\n",
      "weighted avg       0.52      0.52      0.52      9235\n",
      "\n",
      "Confusion Matrix   [[2836 1902]\n",
      " [2525 1972]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(mean_embedded, \n",
    "                  data['loss/profit'],\n",
    "                  XGBClassifier(max_depth=4,\n",
    "                                \n",
    "                                )\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss: 0.7293040620212402\n",
      "Accuracy: 0.5204114780725501\n",
      "Classification report               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.58      0.56      4738\n",
      "           1       0.51      0.45      0.48      4497\n",
      "\n",
      "   micro avg       0.52      0.52      0.52      9235\n",
      "   macro avg       0.52      0.52      0.52      9235\n",
      "weighted avg       0.52      0.52      0.52      9235\n",
      "\n",
      "Confusion Matrix   [[2762 1976]\n",
      " [2453 2044]]\n"
     ]
    }
   ],
   "source": [
    "evaluate_features(mean_embedded, data['loss/profit'],RandomForestClassifier(n_estimators=200, max_depth=30, random_state=45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Keras tokenizer\n",
    "num_words = 2000\n",
    "tokenizer = Tokenizer(num_words=num_words)\n",
    "tokenizer.fit_on_texts(data['Processed_news'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pad the data \n",
    "X = tokenizer.texts_to_sequences(data['Processed_news'].values)\n",
    "X = pad_sequences(X, maxlen=200)\n",
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 200, 138)          276000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 200)               271200    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 547,602\n",
      "Trainable params: 547,602\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build out our simple LSTM\n",
    "embed_dim = 138\n",
    "lstm_out = 200\n",
    "\n",
    "# Model saving callback\n",
    "ckpt_callback = ModelCheckpoint('keras_model', \n",
    "                                 monitor='val_loss', \n",
    "                                 verbose=1, \n",
    "                                 save_best_only=True, \n",
    "                                 mode='auto')\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(num_words, embed_dim, input_length = X.shape[1]))\n",
    "model.add(LSTM( lstm_out,recurrent_dropout=0.2, dropout=0.2,activation='relu'))\n",
    "model.add(Dense(2,activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7388, 200) (7388, 2)\n",
      "(1847, 200) (1847, 2)\n"
     ]
    }
   ],
   "source": [
    "Y = pd.get_dummies(data['loss/profit']).values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42, stratify=Y)\n",
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6649 samples, validate on 739 samples\n",
      "Epoch 1/5\n",
      "6649/6649 [==============================] - 119s 18ms/step - loss: 0.6629 - acc: 0.6630 - val_loss: 0.7977 - val_acc: 0.5034\n",
      "Epoch 2/5\n",
      "6649/6649 [==============================] - 114s 17ms/step - loss: 0.6079 - acc: 0.6745 - val_loss: 0.8322 - val_acc: 0.5189\n",
      "Epoch 3/5\n",
      "6649/6649 [==============================] - 115s 17ms/step - loss: 0.5630 - acc: 0.6860 - val_loss: 0.9357 - val_acc: 0.5108\n",
      "Epoch 4/5\n",
      "6649/6649 [==============================] - 115s 17ms/step - loss: 0.5302 - acc: 0.7033 - val_loss: 1.0672 - val_acc: 0.5027\n",
      "Epoch 5/5\n",
      "6649/6649 [==============================] - 115s 17ms/step - loss: 0.4976 - acc: 0.7282 - val_loss: 1.1746 - val_acc: 0.5149\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x22db2c50>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model.fit(X_train, Y_train, epochs=5, batch_size=batch_size, validation_split=0.1, verbose=1,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('keras_model')\n",
    "model = load_model('keras_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 ... 1 1 1]\n",
      "[1 2 2 ... 2 2 2]\n",
      "Log loss: 1.0431785102003754\n",
      "Accuracy: 0.5230102869518137\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[469, 479],\n",
       "       [402, 497]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_indices = np.argmax(probas, axis=1)\n",
    "print(pred_indices)\n",
    "classes = np.array(range(1,3))\n",
    "preds = classes[pred_indices]\n",
    "print(preds)\n",
    "print('Log loss: {}'.format(log_loss(classes[np.argmax(Y_test, axis=1)], probas)))\n",
    "print('Accuracy: {}'.format(accuracy_score(classes[np.argmax(Y_test, axis=1)], preds)))\n",
    "confusion_matrix(classes[np.argmax(Y_test, axis=1)], preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
